{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe73d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\miniconda3\\envs\\lima\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Training Steps:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Step 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Inference Index 0\n",
      "🔍 Top-K Indices: tensor([ 2, 11,  4,  7,  1,  0,  3,  6, 13, 10,  8,  9, 14, 12, 15,  5],\n",
      "       device='cuda:0')\n",
      "    Demo 0 | Reward: 1.00\n",
      "    Demo 1 | Reward: 1.00\n",
      "    Demo 2 | Reward: 1.00\n",
      "    Demo 3 | Reward: 1.00\n",
      "    Demo 4 | Reward: 0.20\n",
      "    Demo 5 | Reward: 1.00\n",
      "    Demo 6 | Reward: 0.00\n",
      "    Demo 7 | Reward: 0.00\n",
      "    Demo 8 | Reward: 1.00\n",
      "    Demo 9 | Reward: 1.00\n",
      "    Demo 10 | Reward: 0.80\n",
      "    Demo 11 | Reward: 0.40\n",
      "    Demo 12 | Reward: 0.40\n",
      "    Demo 13 | Reward: 0.00\n",
      "    Demo 14 | Reward: 1.00\n",
      "    Demo 15 | Reward: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss: 1811.1215\n",
      "\n",
      "🧠 Inference Index 1\n",
      "🔍 Top-K Indices: tensor([ 5, 11,  3,  7,  1,  0,  2,  6, 13, 10,  8,  9, 14, 12, 15,  4],\n",
      "       device='cuda:0')\n",
      "    Demo 0 | Reward: 1.00\n",
      "    Demo 1 | Reward: 1.00\n",
      "    Demo 2 | Reward: 1.00\n",
      "    Demo 3 | Reward: 1.00\n",
      "    Demo 4 | Reward: 1.00\n",
      "    Demo 5 | Reward: 0.40\n",
      "    Demo 6 | Reward: 0.80\n",
      "    Demo 7 | Reward: 1.00\n",
      "    Demo 8 | Reward: 0.60\n",
      "    Demo 9 | Reward: 0.80\n",
      "    Demo 10 | Reward: 1.00\n",
      "    Demo 11 | Reward: 1.00\n",
      "    Demo 12 | Reward: 1.00\n",
      "    Demo 13 | Reward: 1.00\n",
      "    Demo 14 | Reward: 1.00\n",
      "    Demo 15 | Reward: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss: 5175.2275\n",
      "\n",
      "🧠 Inference Index 2\n",
      "🔍 Top-K Indices: tensor([ 2,  0, 11, 10,  1,  3,  7,  6, 13, 12,  8,  9, 14, 15,  4,  5],\n",
      "       device='cuda:0')\n",
      "    Demo 0 | Reward: 0.00\n",
      "    Demo 1 | Reward: 0.00\n",
      "    Demo 2 | Reward: 0.00\n",
      "    Demo 3 | Reward: 0.00\n",
      "    Demo 4 | Reward: 0.00\n",
      "    Demo 5 | Reward: 0.00\n",
      "    Demo 6 | Reward: 0.00\n",
      "    Demo 7 | Reward: 0.00\n",
      "    Demo 8 | Reward: 0.00\n",
      "    Demo 9 | Reward: 0.00\n",
      "    Demo 10 | Reward: 0.00\n",
      "    Demo 11 | Reward: 0.00\n",
      "    Demo 12 | Reward: 0.00\n",
      "    Demo 13 | Reward: 0.00\n",
      "    Demo 14 | Reward: 0.00\n",
      "    Demo 15 | Reward: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss: 642.3002\n",
      "\n",
      "🧠 Inference Index 3\n",
      "🔍 Top-K Indices: tensor([13, 10,  3,  6,  1,  0,  2,  5, 12,  9,  7,  8, 14, 11, 15,  4],\n",
      "       device='cuda:0')\n",
      "    Demo 0 | Reward: 0.00\n",
      "    Demo 1 | Reward: 0.00\n",
      "    Demo 2 | Reward: 0.00\n",
      "    Demo 3 | Reward: 0.00\n",
      "    Demo 4 | Reward: 0.00\n",
      "    Demo 5 | Reward: 0.00\n",
      "    Demo 6 | Reward: 0.00\n",
      "    Demo 7 | Reward: 0.00\n",
      "    Demo 8 | Reward: 0.00\n",
      "    Demo 9 | Reward: 0.00\n",
      "    Demo 10 | Reward: 0.20\n",
      "    Demo 11 | Reward: 0.00\n",
      "    Demo 12 | Reward: 0.00\n",
      "    Demo 13 | Reward: 0.00\n",
      "    Demo 14 | Reward: 0.00\n",
      "    Demo 15 | Reward: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss: -3606.9187\n",
      "\n",
      "🧠 Inference Index 4\n",
      "🔍 Top-K Indices: tensor([ 8,  0, 16,  2, 10,  9,  6,  7, 12, 11, 13, 14,  5,  4,  1,  3],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Examples:  20%|██        | 4/20 [03:43<14:54, 55.92s/it]\n",
      "Training Steps:   0%|          | 0/1 [03:43<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Demo 0 | Reward: 0.00\n",
      "    Demo 1 | Reward: 0.00\n",
      "    Demo 2 | Reward: 0.00\n",
      "    Demo 3 | Reward: 0.00\n",
      "    Demo 4 | Reward: 0.00\n",
      "    Demo 5 | Reward: 0.00\n",
      "    Demo 6 | Reward: 0.00\n",
      "    Demo 7 | Reward: 0.00\n",
      "    Demo 8 | Reward: 0.00\n",
      "    Demo 9 | Reward: 0.00\n",
      "    Demo 10 | Reward: 0.40\n",
      "    Demo 11 | Reward: 0.00\n",
      "    Demo 12 | Reward: 0.00\n",
      "    Demo 13 | Reward: 0.00\n",
      "    Demo 14 | Reward: 0.20\n",
      "    Demo 15 | Reward: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     74\u001b[0m selected_similarities \u001b[38;5;241m=\u001b[39m similarities[top_k_indices]\n\u001b[1;32m---> 76\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mgrpo_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselected_similarities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdemo_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\D DRIVE\\Sonu\\MINE\\Upenn\\Coursework\\Spring 2025\\CIS6300 Advanced Topics in NLP\\Project Tests\\MATH RL - Updating MathBERT\\grpo_optimizer.py:30\u001b[0m, in \u001b[0;36mgrpo_step\u001b[1;34m(rewards, logp, q_emb, q_ref, optimizer, beta)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m policy_loss \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m kl\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\abhin\\miniconda3\\envs\\lima\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\miniconda3\\envs\\lima\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\miniconda3\\envs\\lima\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mathbert_encoder import MathBERTEncoder\n",
    "import retriever_cosine as rc\n",
    "# from import retrieve_top_k_cosine, retrieve_sample_k_cosine\n",
    "from response_sampler import sample_responses_per_demo\n",
    "from reward_aggregator import compute_demo_accuracy\n",
    "from icl_model_wrapper import OpenAIICLModel\n",
    "from grpo_optimizer import grpo_step\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "\n",
    "reload(rc)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# === Settings ===\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K = 16\n",
    "NUM_SAMPLES_PER_DEMO = 5\n",
    "LEARNING_RATE =  1e-5\n",
    "MAX_STEPS = 1\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# === Init ===\n",
    "encoder = MathBERTEncoder(device=DEVICE, trainable=True)\n",
    "encoder.train()\n",
    "\n",
    "icl_model = OpenAIICLModel(api_key=API_KEY, model_name=\"gpt-4.1-nano\", temperature=TEMPERATURE)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "gsm8k_data = load_dataset('gsm8k', 'main')['train']\n",
    "gsm8k_data = gsm8k_data.select(range(20))  # slice first 200 examples\n",
    "\n",
    "# === Training Loop ===\n",
    "for step in tqdm(range(MAX_STEPS), desc=\"Training Steps\"):\n",
    "    print(f\"\\n=== Training Step {step+1} ===\")\n",
    "\n",
    "    for inference_index in tqdm(range(len(gsm8k_data)), desc=\"Examples\"):\n",
    "        inference_item = gsm8k_data[inference_index]\n",
    "        demo_pool = [d for idx, d in enumerate(gsm8k_data) if idx != inference_index]\n",
    "\n",
    "        Q_inf = inference_item[\"question\"]\n",
    "        A_gt = inference_item[\"answer\"]\n",
    "        demos = [(d[\"question\"], d[\"answer\"]) for d in demo_pool]\n",
    "\n",
    "        q_emb = encoder.encode([Q_inf], detach=False).squeeze(0)\n",
    "        demo_embs = encoder.encode([q for (q, a) in demos], detach=False)\n",
    "\n",
    "        top_k_indices, similarities = rc.retrieve_sample_k_cosine(q_emb, demo_embs, k=min(K, len(demos)))\n",
    "        selected_demos = [demos[i] for i in top_k_indices]\n",
    "\n",
    "        print(f\"\\n🧠 Inference Index {inference_index}\")\n",
    "        print(f\"🔍 Top-K Indices: {top_k_indices}\")\n",
    "\n",
    "        all_responses = sample_responses_per_demo(\n",
    "            demo_tuples=selected_demos,\n",
    "            Q_inf=Q_inf,\n",
    "            icl_model=icl_model,\n",
    "            num_samples=NUM_SAMPLES_PER_DEMO\n",
    "        )\n",
    "\n",
    "        rewards = []\n",
    "        for i, responses in enumerate(all_responses):\n",
    "            reward = compute_demo_accuracy(responses, A_gt)\n",
    "            rewards.append(reward)\n",
    "            print(f\"    Demo {i} | Reward: {reward:.2f}\")\n",
    "\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        selected_similarities = similarities[top_k_indices]\n",
    "\n",
    "        loss = grpo_step(\n",
    "            rewards,\n",
    "            selected_similarities,\n",
    "            q_emb,\n",
    "            demo_embs,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated MathBERT model\n",
    "save_path = \"./updated_mathbert\"  # your save directory\n",
    "encoder.model.save_pretrained(save_path)\n",
    "encoder.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# LOADING\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# model = BertModel.from_pretrained(\"./updated_mathbert\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"./updated_mathbert\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
