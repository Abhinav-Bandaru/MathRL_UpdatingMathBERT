{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3db265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of step_3: 8.666699999999999\n",
      "Sum of step_5: 9.666599999999999\n",
      "Sum of step_10: 9.3333\n",
      "Sum of step_12: 10.6667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "step_3 = [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 1.0000,\n",
    "        0.6667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 1.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.3333, 1.0000]\n",
    "step_5= [0.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000, 1.0000, 0.0000, 0.0000, 0.3333, 0.6667, 0.0000, 0.6667, 0.0000,\n",
    "        0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.3333,\n",
    "        0.0000, 0.3333, 0.0000, 0.0000, 0.3333, 0.3333, 1.0000, 0.0000]\n",
    "step_10 = [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.6667,\n",
    "        0.3333, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.6667, 1.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
    "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000]\n",
    "step_12 = [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.6667, 1.0000,\n",
    "        1.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6667, 0.0000, 0.3333,\n",
    "        0.0000, 0.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000]\n",
    "step_3 = np.array(step_3)\n",
    "step_5 = np.array(step_5)\n",
    "step_10 = np.array(step_10)\n",
    "step_12 = np.array(step_12)\n",
    "\n",
    "print(\"Sum of step_3:\", step_3.sum())\n",
    "print(\"Sum of step_5:\", step_5.sum())\n",
    "print(\"Sum of step_10:\", step_10.sum())\n",
    "print(\"Sum of step_12:\", step_12.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03cd4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error making OpenAI API call: 'OpenAIAdvanced' object has no attribute 'client'\n",
      "Error making OpenAI API call: 'OpenAIAdvanced' object has no attribute 'client'\n",
      "Error making OpenAI API call: 'OpenAIAdvanced' object has no attribute 'client'\n",
      "Error making OpenAI API call: 'OpenAIAdvanced' object has no attribute 'client'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:170\u001b[39m, in \u001b[36mOpenAIAdvanced.query_llm\u001b[39m\u001b[34m(self, prompt, max_tokens, temperature, top_p, max_try_num, model, debug, return_json, json_schema, logprobs, system_prompt_included, is_hippa)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mgpt\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mo3\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:224\u001b[39m, in \u001b[36mOpenAIAdvanced.query_gpt\u001b[39m\u001b[34m(self, prompt, model, max_tokens, temperature, top_p, logprobs, return_json, json_schema, system_prompt_included, is_hippa, debug)\u001b[39m\n\u001b[32m    223\u001b[39m api_params[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = json_schema\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m.beta.chat.completions.parse(**api_params)\n\u001b[32m    225\u001b[39m response = completion.choices[\u001b[32m0\u001b[39m].message.parsed\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAIAdvanced' object has no attribute 'client'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:170\u001b[39m, in \u001b[36mOpenAIAdvanced.query_llm\u001b[39m\u001b[34m(self, prompt, max_tokens, temperature, top_p, max_try_num, model, debug, return_json, json_schema, logprobs, system_prompt_included, is_hippa)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mgpt\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mo3\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:224\u001b[39m, in \u001b[36mOpenAIAdvanced.query_gpt\u001b[39m\u001b[34m(self, prompt, model, max_tokens, temperature, top_p, logprobs, return_json, json_schema, system_prompt_included, is_hippa, debug)\u001b[39m\n\u001b[32m    223\u001b[39m api_params[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = json_schema\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m.beta.chat.completions.parse(**api_params)\n\u001b[32m    225\u001b[39m response = completion.choices[\u001b[32m0\u001b[39m].message.parsed\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAIAdvanced' object has no attribute 'client'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m model = OpenAIAdvanced(api_key=API_KEY)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run generation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m response, mean_logprob = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResponse:\u001b[39m\u001b[33m\"\u001b[39m, response)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMean Logprob:\u001b[39m\u001b[33m\"\u001b[39m, mean_logprob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:134\u001b[39m, in \u001b[36mOpenAIAdvanced.generate\u001b[39m\u001b[34m(self, inference_question, example_questions, logprobs)\u001b[39m\n\u001b[32m    128\u001b[39m detailed_instructions = \u001b[33m\"\u001b[39m\u001b[33mSolve the following math problem, thinking step-by-step. Please make sure to format your reasoning and the final answer in JSON. Keep your reasoning concise and to the point\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m prompt = {\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m# Task Description\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdetailed_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHere are some examples to help you understand the task.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m# Example Solutions\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_questions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    131\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# Solve this problem:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minference_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m response, logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# set True to see prompt / response\u001b[39;49;00m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMathSolutionNumberOnly\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# `query_llm` returns (text, logprobs) when logprobs=True;\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# otherwise it returns just text.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:181\u001b[39m, in \u001b[36mOpenAIAdvanced.query_llm\u001b[39m\u001b[34m(self, prompt, max_tokens, temperature, top_p, max_try_num, model, debug, return_json, json_schema, logprobs, system_prompt_included, is_hippa)\u001b[39m\n\u001b[32m    179\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_try_num >= \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m return_json:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_json\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt_included\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_hippa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     prompt=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mTurn the following text into a JSON object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    183\u001b[39m     json_response = \u001b[38;5;28mself\u001b[39m.query_llm(prompt, model=model, max_tokens=max_tokens, temperature=temperature, top_p=top_p, return_json=\u001b[38;5;28;01mTrue\u001b[39;00m, json_schema=json_schema, logprobs=logprobs, system_prompt_included=\u001b[38;5;28;01mFalse\u001b[39;00m, is_hippa=is_hippa, debug=debug)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MathRL_UpdatingMathBERT/icl_model_wrapper.py:179\u001b[39m, in \u001b[36mOpenAIAdvanced.query_llm\u001b[39m\u001b[34m(self, prompt, max_tokens, temperature, top_p, max_try_num, model, debug, return_json, json_schema, logprobs, system_prompt_included, is_hippa)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError making API call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m curr_try_num += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_try_num >= \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m return_json:\n\u001b[32m    181\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.query_llm(prompt, model=model, max_tokens=max_tokens, temperature=temperature, top_p=top_p, return_json=\u001b[38;5;28;01mFalse\u001b[39;00m, json_schema=json_schema, logprobs=logprobs, system_prompt_included=system_prompt_included, is_hippa=is_hippa, debug=debug)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from icl_model_wrapper import OpenAIAdvanced\n",
    "\n",
    "# Dummy schema to mimic a real pydantic/OpenAI function-calling schema\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# === Settings ===\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Example questions for few-shot prompt\n",
    "example_questions = \"\"\"\n",
    "Problem: What is 5 + 7?\n",
    "Solution: {\"reasoning\": \"Adding 5 and 7 gives 12.\", \"answer\": 12}\n",
    "\n",
    "Problem: What is the square root of 49?\n",
    "Solution: {\"reasoning\": \"The square root of 49 is 7.\", \"answer\": 7}\n",
    "\"\"\"\n",
    "\n",
    "# Inference question\n",
    "inference_question = \"If x = 3, what is the value of x^2 + 2x + 1?\"\n",
    "\n",
    "# Instantiate the class\n",
    "model = OpenAIAdvanced(api_key=API_KEY)\n",
    "\n",
    "\n",
    "# Run generation\n",
    "response, mean_logprob = model.generate(inference_question, example_questions, logprobs=False)\n",
    "\n",
    "print(\"Response:\", response)\n",
    "print(\"Mean Logprob:\", mean_logprob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
