{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc8ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top-K Indices: [81, 159, 35]\n",
      "------------Demo 1--------\n",
      "Q: Irene earns $500 if she works for 40 hours a week and gets an extra $20 for every hour of overtime. If she worked 50 hours last week, calculate her total income.\n",
      "A: If Irene worked 50 hours last week, the total number of hours counting as overtime is 50-40 = <<50-40=10>>10 hours.\n",
      "Since she's given $20 for every hour of overtime, she earned 10*$20 = $<<10*20=200>>200 in overtime.\n",
      "Her total income, including the overtime, is $500+$200= $<<500+200=700>>700\n",
      "#### 700\n",
      "\n",
      "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "A:\n",
      "144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  20%|██        | 1/5 [00:04<00:17,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['144', '72', '144', '72', '72'], ['72', '72', '72', '48 + (48/2) = 48 + 24 = 72', '72'], ['48 + 24 = 72', '48 + 24 = 72', '48 + 24 = 72', '48 + 24 = 72', '48 + (48 / 2) = 48 + 24 = 72']]\n",
      "0.8666666666666667\n",
      "🔍 Top-K Indices: [110, 147, 81]\n",
      "------------Demo 1--------\n",
      "Q: Paul went to a shop to buy some groceries. He bought some bread for $2, butter for $3, and juice for two times the price of the bread. He had $15 for his shopping. How much money did Paul have left?\n",
      "A: The price of the juice was 2 * 2 = $<<2*2=4>>4.\n",
      "So in total he paid for his shopping 2 + 3 + 4 = $<<2+3+4=9>>9.\n",
      "That means he had 15 - 9 = $<<15-9=6>>6 left.\n",
      "#### 6\n",
      "\n",
      "Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "A:\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  40%|████      | 2/5 [00:05<00:08,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10', '10', '$10', '10', '10'], ['10', '10', '10', '10', '10'], ['10', '10', '10', '10', '10']]\n",
      "1.0\n",
      "🔍 Top-K Indices: [201, 73, 228]\n",
      "------------Demo 1--------\n",
      "Q: Susie has $200 in her piggy bank. If she puts 20% more money into her piggy bank, how much money she will have?\n",
      "A: If Susie puts 20% more money into her piggy bank, she'll have 20/100*200 = $<<20/100*200=40>>40 more in her piggy bank.\n",
      "The total amount of money in Susie's piggy bank will increase to $200+$40=$<<200+40=240>>240\n",
      "#### 240\n",
      "\n",
      "Q: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "A:\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  60%|██████    | 3/5 [00:07<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['50', '25', '50', '25', '50'], ['25', '25', '100 - (50 + 15 + 30) = 100 - 95 = 5', '100 - (50 + 15 + 30) = 100 - 95 = 5', '25'], ['25', '25', '$55', '$35', '$55']]\n",
      "1.0\n",
      "🔍 Top-K Indices: [13, 93, 228]\n",
      "------------Demo 1--------\n",
      "Q: Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\n",
      "A: In one hour, there are 3 sets of 20 minutes.\n",
      "So, Joy can read 8 x 3 = <<8*3=24>>24 pages in an hour.\n",
      "It will take her 120/24 = <<120/24=5>>5 hours to read 120 pages.\n",
      "#### 5\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n",
      "A:\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  80%|████████  | 4/5 [00:09<00:02,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['48', '48', '72', '48', '48'], ['48', '48', '48', '48', '54'], ['54', '54', '48', '36', '72']]\n",
      "0.0\n",
      "🔍 Top-K Indices: [93, 13, 113]\n",
      "------------Demo 1--------\n",
      "Q: John writes 20 pages a day.  How long will it take him to write 3 books that are 400 pages each?\n",
      "A: He wants to write 3*400=<<3*400=1200>>1200 pages\n",
      "So it will take him 1200/20=<<1200/20=60>>60 days\n",
      "#### 60\n",
      "\n",
      "Q: James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
      "A:\n",
      "624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval: 100%|██████████| 5/5 [00:11<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['624', '624', '624', '624', '624'], ['624', '312', '312', '312', '312'], ['312', '312', '312', '312', '312']]\n",
      "0.4\n",
      "\n",
      "📊  Baseline accuracy on GSM8K_to_infer: 65.333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0.  Imports & config – nothing here should clash with yours\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mathbert_encoder import MathBERTEncoder\n",
    "import retriever_cosine as rc\n",
    "# from import retrieve_top_k_cosine, retrieve_sample_k_cosine\n",
    "from response_sampler import sample_responses_per_demo\n",
    "from reward_aggregator import compute_demo_accuracy\n",
    "from icl_model_wrapper import OpenAIICLModel\n",
    "from grpo_optimizer import grpo_step\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "\n",
    "reload(rc)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K       = 3                # demos per query\n",
    "NUM_SAMPLES = 5             # model completions per query\n",
    "TEMPERATURE = 0.7           # keep same as training loop\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Initialise encoder **in eval mode** (weights frozen)\n",
    "# ------------------------------------------------------------\n",
    "encoder = MathBERTEncoder(device=DEVICE, trainable=False)\n",
    "encoder.eval()                                   # no grads!\n",
    "\n",
    "icl_model = OpenAIICLModel(api_key=API_KEY,\n",
    "                           model_name=\"gpt-4.1-nano\",\n",
    "                           temperature=TEMPERATURE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Load/define the inference set\n",
    "#     If you already saved a slice elsewhere, just load it.\n",
    "# ------------------------------------------------------------\n",
    "gsm8k_data = load_dataset(\"gsm8k\", \"main\")[\"train\"].select(range(256))  # slice first 200 examples\n",
    "gsm8k_to_infer = load_dataset(\"gsm8k\", \"main\")[\"train\"] \\\n",
    "                     .select(range(5))          # ⬅ change as needed\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Evaluation loop\n",
    "# ------------------------------------------------------------\n",
    "total, correct = 0, 0\n",
    "all_losses     = []          # optional – to compare prompt quality\n",
    "\n",
    "for idx in tqdm(range(len(gsm8k_to_infer)), desc=\"Baseline eval\"):\n",
    "    item       = gsm8k_to_infer[idx]\n",
    "    Q_inf, A_gt = item[\"question\"], item[\"answer\"]\n",
    "\n",
    "    # Build demo pool (everything except current query)\n",
    "    demos = [(d[\"question\"], d[\"answer\"])\n",
    "             for j, d in enumerate(gsm8k_data) if j != idx]\n",
    "\n",
    "    # Encode query + candidate demos\n",
    "    with torch.no_grad():\n",
    "        q_emb     = encoder.encode([Q_inf], detach=True).squeeze(0)\n",
    "        demo_embs = encoder.encode([q for (q, _) in demos], detach=True)\n",
    "\n",
    "    # ------- ORIGINAL cosine retrieval -------------------------\n",
    "    top_k, _ = rc.retrieve_top_k_cosine(\n",
    "        q_emb, demo_embs, k=min(K, len(demos))\n",
    "    )\n",
    "    print(f\"🔍 Top-K Indices: {top_k}\")\n",
    "    selected_demos = [demos[i] for i in top_k]          # length = 2\n",
    "\n",
    "    # ------- Run the ICL model --------------------------------\n",
    "    responses_nested = sample_responses_per_demo(\n",
    "        demo_tuples = selected_demos,\n",
    "        Q_inf       = Q_inf,\n",
    "        icl_model   = icl_model,\n",
    "        num_samples = NUM_SAMPLES,\n",
    "        parallel=True\n",
    "    )\n",
    "    print(responses_nested)\n",
    "    flat_responses = [r for demo_resps in responses_nested for r in demo_resps]\n",
    "    acc = compute_demo_accuracy(flat_responses, A_gt)\n",
    "    print(acc) \n",
    "    correct += acc\n",
    "    total   += 1\n",
    "\n",
    "baseline_acc = correct / total\n",
    "print(f\"\\n📊  Baseline accuracy on GSM8K_to_infer: {baseline_acc:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe73d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Step 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Question---------\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "🧠 Inference Index 0\n",
      "🔍 Top-K Indices: [176, 50, 159, 81, 163, 109, 84, 247, 203, 52, 51, 201, 202, 161, 89, 105, 56, 250, 104, 254, 110, 4, 86, 144, 95, 59, 0, 29, 8, 41, 2, 167]\n",
      "------------Demo 1--------\n",
      "Q: Samantha’s last name has three fewer letters than Bobbie’s last name. If Bobbie took two letters off her last name, she would have a last name twice the length of Jamie’s. Jamie’s full name is Jamie Grey. How many letters are in Samantha’s last name?\n",
      "A: There are 4 letters in Jamie’s last name, so Bobbie’s name is 4*2 +2 = <<4*2+2=10>>10 letters long.\n",
      "Samantha’s last name is 3 letters shorter than Bobbie’s, so there are 10 - 3 = <<10-3=7>>7 letters in Samantha’s last name.\n",
      "#### 7\n",
      "\n",
      "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "A:\n",
      "48 + 24 = 72\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mathbert_encoder import MathBERTEncoder\n",
    "import retriever_cosine as rc\n",
    "# from import retrieve_top_k_cosine, retrieve_sample_k_cosine\n",
    "from response_sampler import sample_responses_per_demo\n",
    "from reward_aggregator import compute_demo_accuracy\n",
    "from icl_model_wrapper import OpenAIICLModel\n",
    "from grpo_optimizer import grpo_step\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "\n",
    "reload(rc)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# === Settings ===\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K = 32\n",
    "NUM_SAMPLES_PER_DEMO = 5\n",
    "LEARNING_RATE =  1e-5\n",
    "MAX_STEPS = 3\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# === Init ===\n",
    "encoder = MathBERTEncoder(device=DEVICE, trainable=True)\n",
    "encoder.train()\n",
    "\n",
    "icl_model = OpenAIICLModel(api_key=API_KEY, model_name=\"gpt-4.1-nano\", temperature=TEMPERATURE)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "gsm8k_data = load_dataset('gsm8k', 'main')['train']\n",
    "gsm8k_data = gsm8k_data.select(range(256))  # slice first 200 examples\n",
    "gsm8k_data_to_infer = gsm8k_data.select(range(0,5))  # slice first 200 examples\n",
    "\n",
    "# === Training Loop ===\n",
    "for step in tqdm(range(MAX_STEPS), desc=\"Training Steps\"):\n",
    "    print(f\"\\n=== Training Step {step+1} ===\")\n",
    "\n",
    "    for inference_index in tqdm(range(len(gsm8k_data_to_infer)), desc=\"Examples\"):\n",
    "        inference_item = gsm8k_data_to_infer[inference_index]\n",
    "        demo_pool = [d for idx, d in enumerate(gsm8k_data) if idx != inference_index]\n",
    "\n",
    "        Q_inf = inference_item[\"question\"]\n",
    "        A_gt = inference_item[\"answer\"]\n",
    "        demos = [(d[\"question\"], d[\"answer\"]) for d in demo_pool]\n",
    "\n",
    "        q_emb = encoder.encode([Q_inf], detach=False).squeeze(0)\n",
    "        demo_embs = encoder.encode([q for (q, a) in demos], detach=False)\n",
    "        \n",
    "        top_k_indices, _ = rc.retrieve_top_k_cosine(q_emb, demo_embs, k=min(K, len(demos)))\n",
    "        print(\"-------Question---------\")\n",
    "        print(Q_inf)\n",
    "        print(f\"\\n🧠 Inference Index {inference_index}\")\n",
    "        print(f\"🔍 Top-K Indices: {top_k_indices}\")\n",
    "        \n",
    "        top_k_indices, similarities = rc.retrieve_sample_k_cosine(q_emb, demo_embs, k=min(K, len(demos)))\n",
    "        selected_demos = [demos[i] for i in top_k_indices]\n",
    "\n",
    "\n",
    "\n",
    "        all_responses = sample_responses_per_demo(\n",
    "            demo_tuples=selected_demos,\n",
    "            Q_inf=Q_inf,\n",
    "            icl_model=icl_model,\n",
    "            num_samples=NUM_SAMPLES_PER_DEMO,\n",
    "            parallel=True\n",
    "        )\n",
    "\n",
    "        rewards = []\n",
    "        for i, responses in enumerate(all_responses):\n",
    "            reward = compute_demo_accuracy(responses, A_gt)\n",
    "            rewards.append(reward)\n",
    "            # print(f\"    Demo {i} | Reward: {reward:.2f}\")\n",
    "\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        loss = grpo_step(\n",
    "            rewards,\n",
    "            similarities,\n",
    "            q_emb,\n",
    "            demo_embs,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7be05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top-K Indices: [49, 138, 8]\n",
      "------------Demo 1--------\n",
      "Q: Gerald spends $100 a month on baseball supplies. His season is 4 months long. He wants to use the months he's not playing baseball to save up by raking, shoveling, and mowing lawns. He charges $10 for each. How many chores does he need to average a month to save up for his supplies?\n",
      "A: He needs to save up $400 because 4 x 100 = <<4*100=400>>400\n",
      "He has 8 months to earn this money because 12 - 4 = <<12-4=8>>8\n",
      "He needs to earn $50 a month because 400 / 8 = <<400/8=50>>50\n",
      "He needs to do 5 tasks a month because 50 / 10 = <<50/10=5>>5\n",
      "#### 5\n",
      "\n",
      "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "A:\n",
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  20%|██        | 1/5 [00:01<00:06,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['72', '72', '72', '72', '72'], ['48 + 24 = 72', '48 + (48 / 2) = 48 + 24 = 72', '48 + 24 = 72', '48 + 24 = 72', '48 + (48 / 2) = 48 + 24 = 72'], ['72', '72', '72', '72', '144']]\n",
      "0.9333333333333333\n",
      "🔍 Top-K Indices: [12, 177, 221]\n",
      "------------Demo 1--------\n",
      "Q: Jasper will serve charcuterie at his dinner party. He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese. How much does he spend on the ingredients?\n",
      "A: A pound of cream cheese cost $10 / 2 = $<<10/2=5>>5.\n",
      "A pack of cold cuts cost $10 x 2 = $<<10*2=20>>20.\n",
      "Jasper spent $10 + $5 + $20 = $<<10+5+20=35>>35 on the ingredients.\n",
      "#### 35\n",
      "\n",
      "Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "A:\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  40%|████      | 2/5 [00:03<00:04,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10', '10', '10', '10', '10'], ['10', '10', '10', '10', '10'], ['10', '10', '10', '10', '10']]\n",
      "1.0\n",
      "🔍 Top-K Indices: [177, 37, 176]\n",
      "------------Demo 1--------\n",
      "Q: Simon wanted to buy flowers that his mom could plant for Mother's Day.  The garden center was offering 10% off all purchases.  He bought 5 pansies at $2.50 each, one hydrangea that cost $12.50 and 5 petunias that cost $1.00 each.  If he paid with a $50 bill, how much change would Simon receive back from his purchase?\n",
      "A: 5 pansies at $2.50 each is 5*2.50 = $<<5*2.5=12.50>>12.50\n",
      "5 petunias at $1.00 each 5*1 = $<<5*1=5.00>>5.00\n",
      "All total he spends 12.50+12.50+5.00 = $<<12.50+12.50+5.00=30.00>>30.00\n",
      "The sale is 10% off so 30*.10 = $<<30*.10=3.00>>3.00\n",
      "The purchase total now comes to 30-3 = $<<30-3=27.00>>27.00\n",
      "He pays with a $50 bill so 50-27 = $<<50-27=23.00>>23.00\n",
      "#### 23\n",
      "\n",
      "Q: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "A:\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  60%|██████    | 3/5 [00:05<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['50', '50', '50', '50', '50'], ['55', '25', '25', '25', '70'], ['55', '100 - (50 + 15 + 30) = 100 - 95 = 5', '55', '55', '55']]\n",
      "0.9333333333333333\n",
      "🔍 Top-K Indices: [5, 49, 8]\n",
      "------------Demo 1--------\n",
      "Q: Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?\n",
      "A: He eats 32 from the largest pizzas because 2 x 16 = <<2*16=32>>32\n",
      "He eats 16 from the small pizza because 2 x 8 = <<2*8=16>>16\n",
      "He eats 48 pieces because 32 + 16 = <<32+16=48>>48\n",
      "#### 48\n",
      "\n",
      "Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n",
      "A:\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval:  80%|████████  | 4/5 [00:07<00:02,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['54', '36', '48', '33', '54'], ['48', '48', '48', '48', '48'], ['54', '36', '54', '54', '54']]\n",
      "0.0\n",
      "🔍 Top-K Indices: [206, 156, 175]\n",
      "------------Demo 1--------\n",
      "Q: John volunteers at a shelter twice a month for 3 hours at a time.  How many hours does he volunteer per year?\n",
      "A: He volunteers 2*12=<<2*12=24>>24 times a year\n",
      "So he volunteers for 24*3=<<24*3=72>>72 hours\n",
      "#### 72\n",
      "\n",
      "Q: James writes a 3-page letter to 2 different friends twice a week.  How many pages does he write a year?\n",
      "A:\n",
      "624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline eval: 100%|██████████| 5/5 [00:09<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['624', '312', '624', '312', '624'], ['312', '312', '312', '312', '312'], ['3 pages × 2 friends × 2 times a week × 52 weeks = 3 × 2 × 2 × 52 = 624', '312', '312', '312', '312']]\n",
      "0.26666666666666666\n",
      "\n",
      "📊  Baseline accuracy on GSM8K_to_infer: 62.667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0.  Imports & config – nothing here should clash with yours\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from mathbert_encoder import MathBERTEncoder\n",
    "import retriever_cosine as rc\n",
    "# from import retrieve_top_k_cosine, retrieve_sample_k_cosine\n",
    "from response_sampler import sample_responses_per_demo\n",
    "from reward_aggregator import compute_demo_accuracy\n",
    "from icl_model_wrapper import OpenAIICLModel\n",
    "from grpo_optimizer import grpo_step\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from importlib import reload\n",
    "\n",
    "reload(rc)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "K       = 3                # demos per query\n",
    "NUM_SAMPLES = 5             # model completions per query\n",
    "TEMPERATURE = 0.7           # keep same as training loop\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Initialise encoder **in eval mode** (weights frozen)\n",
    "# ------------------------------------------------------------\n",
    "encoder.eval()                                   # no grads!\n",
    "\n",
    "icl_model = OpenAIICLModel(api_key=API_KEY,\n",
    "                           model_name=\"gpt-4.1-nano\",\n",
    "                           temperature=TEMPERATURE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Load/define the inference set\n",
    "#     If you already saved a slice elsewhere, just load it.\n",
    "# ------------------------------------------------------------\n",
    "gsm8k_data = load_dataset(\"gsm8k\", \"main\")[\"train\"].select(range(256))  # slice first 200 examples\n",
    "gsm8k_to_infer = load_dataset(\"gsm8k\", \"main\")[\"train\"] \\\n",
    "                     .select(range(5))          # ⬅ change as needed\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Evaluation loop\n",
    "# ------------------------------------------------------------\n",
    "total, correct = 0, 0\n",
    "all_losses     = []          # optional – to compare prompt quality\n",
    "\n",
    "for idx in tqdm(range(len(gsm8k_to_infer)), desc=\"Baseline eval\"):\n",
    "    item       = gsm8k_to_infer[idx]\n",
    "    Q_inf, A_gt = item[\"question\"], item[\"answer\"]\n",
    "\n",
    "    # Build demo pool (everything except current query)\n",
    "    demos = [(d[\"question\"], d[\"answer\"])\n",
    "             for j, d in enumerate(gsm8k_data) if j != idx]\n",
    "\n",
    "    # Encode query + candidate demos\n",
    "    with torch.no_grad():\n",
    "        q_emb     = encoder.encode([Q_inf], detach=True).squeeze(0)\n",
    "        demo_embs = encoder.encode([q for (q, _) in demos], detach=True)\n",
    "\n",
    "    # ------- ORIGINAL cosine retrieval -------------------------\n",
    "    top_k, _ = rc.retrieve_top_k_cosine(\n",
    "        q_emb, demo_embs, k=min(K, len(demos))\n",
    "    )\n",
    "    print(f\"🔍 Top-K Indices: {top_k}\")\n",
    "    selected_demos = [demos[i] for i in top_k]          # length = 2\n",
    "\n",
    "    # ------- Run the ICL model --------------------------------\n",
    "    responses_nested = sample_responses_per_demo(\n",
    "        demo_tuples = selected_demos,\n",
    "        Q_inf       = Q_inf,\n",
    "        icl_model   = icl_model,\n",
    "        num_samples = NUM_SAMPLES,\n",
    "        parallel=True\n",
    "    )\n",
    "    print(responses_nested)\n",
    "    flat_responses = [r for demo_resps in responses_nested for r in demo_resps]\n",
    "    acc = compute_demo_accuracy(flat_responses, A_gt)\n",
    "    print(acc) \n",
    "    correct += acc\n",
    "    total   += 1\n",
    "\n",
    "baseline_acc = correct / total\n",
    "print(f\"\\n📊  Baseline accuracy on GSM8K_to_infer: {baseline_acc:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated MathBERT model\n",
    "save_path = \"./updated_mathbert\"  # your save directory\n",
    "encoder.model.save_pretrained(save_path)\n",
    "encoder.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# LOADING\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# model = BertModel.from_pretrained(\"./updated_mathbert\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"./updated_mathbert\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
